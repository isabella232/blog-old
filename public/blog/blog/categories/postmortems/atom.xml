<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: postmortems | Rollbar - Blog - real-time error tracking for Rails, Python, PHP, Javascript, and Flash]]></title>
  <link href="https://rollbar.com/blog/blog/categories/postmortems/atom.xml" rel="self"/>
  <link href="https://rollbar.com/blog/"/>
  <updated>2014-04-11T10:56:47-07:00</updated>
  <id>https://rollbar.com/blog/</id>
  <author>
    <name><![CDATA[Rollbar, Inc.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[4/10/2014 Processing Delay Postmortem]]></title>
    <link href="https://rollbar.com/blog/post/2014/04/11/processing-delay-postmortem/"/>
    <updated>2014-04-11T10:48:00-07:00</updated>
    <id>https://rollbar.com/blog/post/2014/04/11/processing-delay-postmortem</id>
    <content type="html"><![CDATA[<p>Yesterday from about 2:30pm PDT until 4:55pm PDT, we experienced a service degradation that caused our customers to see processing delays up to about 2 hours. While no data was lost, alerts were not being sent and new data was not appearing in the rollbar.com interface. Customers instead would see alerts notices on the Dashboard and Items page about the delay.</p>

<p>We know that you rely on Rollbar to monitor your applications and alert you when things go wrong, and we are very sorry that we let you down during this outage.</p>

<p>The service degradation began following some planned database maintenance, which we had expected to have no significant impact on service.</p>

<h2>The Planned Maintenance</h2>

<p>We store all of our data in MySQL in a master-master/active-passive configuration. Yesterday we needed to add partitions to our largest table - a routine procedure. Normally, this process takes about 15 minutes, during which time customers experience small delays in data processing. This process generally goes unnoticed by customers. However, this time something caused the database to load new data extremely slowly which, in turn, caused the outage.</p>

<h2>Timeline</h2>

<ul>
<li><p>2:06pm - We began the planned maintenance by promoting the passive master to be the new active master.</p></li>
<li><p>2:29pm - The planned maintenance was complete.</p>

<p>  All connections to our old active master were closed and the new active master was getting new data and processing it.</p></li>
<li><p>2:40pm - It became apparent that new data was being loaded and processed very slowly.</p>

<p>  We turned off our data loaders to decrease any contention in the database.</p></li>
<li><p>2:41pm - We began profiling the slow worker.</p></li>
<li><p>2:47pm - We tested a theory that a single recurring item was causing most of the slow processing.</p></li>
<li><p>~2:50pm - We noticed that ping times to the new active master were 1-5 milliseconds - an order of magnitude slower than normal.</p></li>
<li><p>2:52pm - We turned off replication from the passive to the active database which seemed to help loading data, but not by much.</p></li>
<li><p>~2:50pm - 3pm</p>

<ul>
<li><p>The <code>ALTER</code> to add partitions completed on the passive database.</p>

<ul>
<li>Up to this point, there were no connections on the passive database.</li>
</ul>
</li>
<li><p>We decided to switch back to the previous active master but quickly reverted after finding that our passive database was missing a significant number of rows.</p>

<ul>
<li>MySQL replication was 0 seconds behind.</li>
<li>It was unclear how the passive database thought it was caught up but was missing data.</li>
</ul>
</li>
</ul>
</li>
<li><p>~3:15pm - We identified a the slowest portion of the slow worker, which happened to be unused. We removed this code and deployed to all workers.</p>

<ul>
<li>This got processing back to normal speeds and allowed us to begin catching back up.</li>
</ul>
</li>
<li><p>~3:30pm - We turned the data loaders back on.</p></li>
<li><p>4:30pm - The worker responsible to making new occurrences appear in the interface was caught up, but notifications were still delayed.</p></li>
<li><p>4:55pm - Everything was caught back up and we were back to 0 delay.</p></li>
</ul>


<h2>Follow-on Tasks</h2>

<p>We have two open questions:</p>

<ol>
<li>Why did the data loaders slow down when we switched to the new active master?</li>
<li>How did the databases get out of sync?</li>
</ol>


<p>We have some theories as to why the data loaders slowed down so much but we are not sure. It could have been the amount of concurrent processes trying to load data into the same table. It could also have been something about the disk layout or cache on the new active master. We plan to investigate serializing loads in general and/or slowly ramping up loads after maintenance in the future.</p>

<p>To determine why our databases became out of sync, we wrote a script to tell us the exact moment when they diverged. Once it completes we will find the coordinates in the new active master's binlogs that correspond with the point in time where the databases became out of sync, then restart replication on the passive master using those coordinates.</p>

<h2>Conclusion</h2>

<p>We take downtime very seriously and we want to be as transparent as possible when it happens.</p>

<p>We are sorry for the degradation of service and we are working on making sure it doesn't happen again. If you have any questions, please don't hesitate to contact us as <a href="support@rollbar.com">support@rollbar.com</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Post-mortem from last night's outage]]></title>
    <link href="https://rollbar.com/blog/post/2013/01/11/post-mortem-from-last-nights-outage/"/>
    <updated>2013-01-11T15:57:00-08:00</updated>
    <id>https://rollbar.com/blog/post/2013/01/11/post-mortem-from-last-nights-outage</id>
    <content type="html"><![CDATA[<blockquote><p><em>Tl;dr: from about 9:30pm to 12:30am last night, our website was unreachable and we weren't sending out any notifications. Our API stayed up nearly the whole time thanks to an automatic failover.</em></p></blockquote>

<p>We had our first major outage last night. We want to apologize to all of our customers for this outage, and we're going to continue to work to make the <a href="http://rollbar.com">Rollbar.com</a> service stable, reliable, and performant.</p>

<p>What follows is a timeline of events, and a summary of what went wrong, what went right, and what we're doing to address what went wrong.</p>

<h2>Background</h2>

<p>First some background: our infrastructure is currently hosted at Softlayer and layed out like this (simplified):</p>

<p><img src="https://d37gvrvc0wt4s1.cloudfront.net/static/img/blog/infrastructurediagram.png"></p>

<p>That is:</p>

<ul>
<li>our primary cluster of servers is in San Jose</li>
<li>all web traffic (rollbar.com / www.rollbar.com) is handled by lb2</li>
<li>all API traffic (api.rollbar.com) is handled by lb1</li>
<li>lb3 (in Singapore) and lb4 (Amsterdam) are ready to go but not in use yet (more on this below), providing failover and faster API response times to customers outside of the Americas.</li>
</ul>


<p>We've been in the process of setting up lb3 and lb4, along with some <a href="http://dyn.com/dns/dynect-managed-dns/">fancy DNS functionality</a> from Dyn, to provide redundancy and faster response times to our customers outside of the Americas. Each is running a stripped-down version of our infrastructure, including:</p>

<ul>
<li>a frontend web server (nginx)</li>
<li>two instsances of our node.js API server</li>
<li>a partial database slave (for validating access tokens)</li>
<li>our offline loading process (soon to be open-sourced!), for doing async writes to the active master database.</li>
</ul>


<p>Switching DNS to Dyn requires changing the nameservers, which can take "up to 48 hours". At the start of this story, it's been about 36 hours. To play it safe, after testing out Dyn on a separate domain, we configured it to have the same settings as we had before -- lb3 and lb4 are not in play yet.</p>

<h2>Timeline</h2>

<p>Now the (abbreviated) timeline. All times are PST.</p>

<div style="padding-left:2em;">
<p>9:30pm: Cory got an alert from Pingdom that our website (rollbar.com) was down. He tried visiting it but it wouldn't load (just hung). Remembering the pending DNS change, he immediately checked DNS propagation and saw that rollbar.com was pointing at the wrong load balancer -- lb1 (the API tier), not lb2.

<p>Cory and Sergei investigated. The A record for rollbar.com showed as correct in Dyn, but DNS was resolving incorrectly.

<p>9:47pm: Cory and Sergei looked at <a href="http://twitter.com/SoftlayerNotify" target="_blank">@SoftlayerNotify</a> and saw that there was an issue underway with one of the routers in the San Jose data center.

<p>9:49pm: Website accessible by its IP address.

<p>9:51pm: No longer accessible by IP.

<p>10:05pm: Twitter search for "softlayer outage" shows other people being affected.

<p>10:05pm: API tier (api.rollbar.com) appears to be working. Sergei verifies that it's hitting lb3 (in Singapore).
</div>


<p>You might notice that we said before that lb3 wasn't supposed to be in service yet. What appeared to have happened DNS had automatically failed over to lb3 (since lb1 was down because of the Softlayer outage). We had set something like this up before when testing out Dyn, but it wasn't supposed to be active yet. Fortunately, lb3 was ready to go and handled all of our API load just fine.</p>

<div style="padding-left:2em;">
<p>10:22pm: Sergei tries fiddling with the Dyn configuration to see if anything helps.

<p>10:35pm: Sergei starts trying to get ahold Dyn

<p>10:58pm: Softlayer posts that "13 out of 14 rows of servers are online". We must be in the 14th, because we're still unreachable at this point. Brian tries hard-rebooting the 'dev' server to see if it helps. It doesn't.

<p>11:15pm: Sergei gets a call from Dyn, who tells him that the problem was a "stale Real-Time Traffic Manager configuration" and they're looking into it.

<p>11:54pm: @SoftlayerNotify posts that "all servers are online however some intermittent problems remain"

<p>11:55pm: Sergei notices that the A record for rollbar.com in the Dyn interface appears to have been deleted, and he can't add it back.

<p>12:00am: Brian sees that rollbar.com is working again. Cory notices that API calls are hitting lb2, causing them to hit the old, non-optimized API handling code on our web tier, overloading them and causing the website to hang. Frequent process restarts minimize the impact.

<p>12:19am: Sergei gets an email back from Dyn saying that they're still looking into the problem.

<p>12:28am: Dyn calls to say they were able to fix everything. Sergei confirms. lb3 and lb4 are now fully utilized.

<p>12:42am: Brian tweets that all systems are stable.

<p>2:58am: Softlayer tweets that they're about to run some code upgrades on the troubled router, which will cause some public network disruption.

<p>4:00am:- A customer reports connectivity issues to rollbar.com

<p>4:10am: Softlayer tweets that the troubled router is finally stable.
</div>


<h2>So what happened here?</h2>

<ol>
<li>Softlayer experienced a network outage, causing our servers in San Jose to be intermittently, then fully, unreachable</li>
<li>This triggered a DNS failover controlled by a stale Dyn configuration, which cascaded into a broken set of DNS records</li>
<li>After about 3 hours, our San Jose servers came back online, and about 30 minutes after that, the DNS issue was resolved.</li>
</ol>


<h2>What went right</h2>

<ul>
<li>We were notified of the problem by our backup monitoring service, Pingdom. (We're using Nagios as our primary, but it runs inside of San Jose.)</li>
<li>Dyn's DNS failover did work, even though wasn't really supposed to be turned on. Our logs don't show any large gaps in customer data being received.</li>
<li>A single machine (lb3) was able to handle all of our API traffic during the outage.</li>
<li>The API tier was able to handle a master-offline situation.</li>
<li>When San Jose came back online, data processing quickly caught up, notifications were sent, and the system was stable.</li>
<li>Our team came together, stayed mostly calm, and did everything we reasonably could to restore service as quickly as possible.</li>
</ul>


<p>As a bonus, our Singapore and Amsterdam servers are <a href="http://www.whatsmydns.net/#A/api.rollbar.com">now in service</a>.</p>

<h2>What went wrong</h2>

<ul>
<li>Parts of our service were unusable for a long period of time

<ul>
<li>Notifications for new errors, etc. weren't sent</li>
<li>The web app didn't load, and there was no maintenance page.</li>
<li><a href="http://status.rollbar.com">status.rollbar.com</a> didn't show useful information</li>
</ul>
</li>
<li>Even though the Softlayer private network was at least partially accessible, we couldn't access it because we only had one way in ('dev', in San Jose).</li>
<li>The web tier got crushed trying to handle the API load with its old code.</li>
</ul>


<h2>Action items</h2>

<p>In the short term (most of this will get done today):</p>

<div style="padding-left:2em;">
<p><i>1b.</i> Set up a web server in a separate datacenter to serve a maintenance page.

<p><i>1c.</i> Add meta-level checks to status.rollbar.com. It currently gets data pushed from Nagios, but this isn't helpful when San Jose is entirely unreachable.

<p><i>2.</i> Add another 'dev'-like machine that we can use to administer servers, deploy code, etc. if San Jose is unreachable

<p><i>3.</i> Remove that old code, and make it an error if any API traffic hits the web tier.
</div>


<p>And longer term:</p>

<div style="padding-left:2em;">
<p><i>1a.</i> Add a host master standby in another datacenter for fast failover. If an episode like last night's happens again, this will let us get notifications back online in a few minutes instead of a few hours.

<p><i>1b.</i> Set up a read-only web tier in another datacenter
</div>


<h2>Conclusion</h2>

<p>We hope this was, if nothing else, an interesting look into our infrastructure, and to the journey of building a highly-available we service.</p>

<p>If you have any questions about the outage or otherwise, let us know in the comments or email us at support@rollbar.com</p>
]]></content>
  </entry>
  
</feed>
